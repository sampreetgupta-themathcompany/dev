{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tags to identify this iteration when submitted\n",
    "# example: codex_tags = {'env': 'dev', 'region': 'USA', 'product_category': 'A'}\n",
    "\n",
    "codex_tags = {\n",
    "}\n",
    "\n",
    "from codex_widget_factory import utils\n",
    "results_json=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BEGIN CUSTOM CODE BELOW...\n",
    "\n",
    "#put your output in this response param for connecting to downstream widgets\n",
    "step_1 = \"\"\"\n",
    "# Below codestring is used to perform detailed analysis on quantity of sales done over time. \n",
    "\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import json\n",
    "from io import StringIO\n",
    "import plotly.io as io\n",
    "\n",
    "\n",
    "def getLogger():\n",
    "    import logging\n",
    "    logging.basicConfig(filename=\"UIACLogger.log\",\n",
    "                        format='%(asctime)s %(message)s',\n",
    "                        filemode='a')\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    return logger\n",
    "\n",
    "\n",
    "logger = getLogger()\n",
    "\n",
    "\n",
    "def fiile_read(blob_name):\n",
    "    #reading the file from azure blob storage container\n",
    "    logger.info(f\"Read dataset file: {blob_name}\")\n",
    "    try:\n",
    "        from azure.storage.blob import BlockBlobService\n",
    "        sas_token = '?sv=2021-04-10&st=2022-12-22T08%3A12%3A47Z&se=2023-12-30T08%3A12%3A00Z&sr=c&sp=racwl&sig=fMeYkXsCvwK%2F0qVrCmj2j3NiMricQjOPWOkAEXekIPA%3D'\n",
    "        account_name = 'willbedeletedsoon'\n",
    "        container_name = 'codx-pede-s02'\n",
    "        #blob_name = 'cost-of-living_v2_I0869'\n",
    "        def get_data_from_blob(sas_token, account_name, container_name, blob_name):\n",
    "            block_blob_service = BlockBlobService(account_name=account_name, sas_token= sas_token)\n",
    "            from_blob = block_blob_service.get_blob_to_text(container_name = container_name, blob_name=blob_name)\n",
    "            return pd.read_csv(StringIO(from_blob.content))\n",
    "        input_df=get_data_from_blob(sas_token, account_name, container_name, blob_name)\n",
    "        #filtering out empty columns\n",
    "        input_df = input_df[input_df.x2.notnull()]\n",
    "        input_df = input_df[input_df.x23.notnull()]\n",
    "        input_df = input_df[input_df.x33.notnull()]\n",
    "        input_df = input_df[input_df.x54.notnull()]\n",
    "        data_good = input_df[input_df[\"data_quality\"] == 1]\n",
    "        \n",
    "        ##print(data_good)\n",
    "        return(input_df)\n",
    "    except Exception as error_msg:\n",
    "        logger.info(f\"Exception occured while reading the dataset: {blob_name}\"\n",
    "                    f\"Error Info is  {error_msg}\")\n",
    "\n",
    "\n",
    "def getGraph(dframe, filters):\n",
    "    logger.info(\n",
    "        \"Preparing bar graph json to understand basic_expenditure across all countries\")\n",
    "    column_names = ['x48', 'x38', 'x37', 'x36', 'x33', 'x8', 'x29']\n",
    "    #print(dframe['country'])\n",
    "    data_good = dframe[dframe[\"data_quality\"] == 1]\n",
    "    data_good['basic_expenditure']= data_good[column_names].sum(axis=1)\n",
    "    for item in filters:\n",
    "        if 'All' in filters[item]:\n",
    "            continue\n",
    "        elif isinstance(filters[item], list):\n",
    "            data_good = data_good[data_good[item].isin(filters[item])]\n",
    "        else:\n",
    "            data_good = data_good[data_good[item] == filters[item]]\n",
    "    fig = px.bar(data_good, x='x48', y='country', color='basic_expenditure')\n",
    "    #fig.show()\n",
    "    logger.info(\n",
    "        \"Successfully prepared bar graph json to understand basic_expenditure across all countries\")\n",
    "    return io.to_json(fig)\n",
    "\n",
    "\n",
    "selected_filters = {\"country\": ['India','Denmark']}\n",
    "dframe = fiile_read(\"cost-of-living_v2_I0869.csv\")\n",
    "#dframe = dframe.groupby(['country'])\n",
    "dynamic_outputs = getGraph(dframe, selected_filters)#here11\n",
    "\n",
    "\"\"\"\n",
    "# #END CUSTOM CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BEGIN CUSTOM CODE BELOW...\n",
    "\n",
    "#put your output in this response param for connecting to downstream widgets\n",
    "step_2_Filters = \"\"\"\n",
    "## Below codestring is used to create filters to show different regions data in  product historic screen.\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "from io import StringIO\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "def getLogger():\n",
    "    import logging\n",
    "    logging.basicConfig(filename=\"UIACLogger.log\",\n",
    "                        format='%(asctime)s %(message)s',\n",
    "                        filemode='a')\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    return logger\n",
    "\n",
    "\n",
    "logger = getLogger()\n",
    "\n",
    "\n",
    "def fiile_read(blob_name):\n",
    "    #reading the file from azure blob storage container\n",
    "    logger.info(f\"Read dataset file: {blob_name}\")\n",
    "    try:\n",
    "        from azure.storage.blob import BlockBlobService\n",
    "        sas_token = '?sv=2021-04-10&st=2022-12-22T08%3A12%3A47Z&se=2023-12-30T08%3A12%3A00Z&sr=c&sp=racwl&sig=fMeYkXsCvwK%2F0qVrCmj2j3NiMricQjOPWOkAEXekIPA%3D'\n",
    "        account_name = 'willbedeletedsoon'\n",
    "        container_name = 'codx-pede-s02'\n",
    "        #blob_name = 'cost-of-living_v2_I0869'\n",
    "        def get_data_from_blob(sas_token, account_name, container_name, blob_name):\n",
    "            block_blob_service = BlockBlobService(account_name=account_name, sas_token= sas_token)\n",
    "            from_blob = block_blob_service.get_blob_to_text(container_name = container_name, blob_name=blob_name)\n",
    "            return pd.read_csv(StringIO(from_blob.content))\n",
    "        input_df=get_data_from_blob(sas_token, account_name, container_name, blob_name)\n",
    "        #filtering out empty columns\n",
    "        input_df = input_df[input_df.x2.notnull()]\n",
    "        input_df = input_df[input_df.x23.notnull()]\n",
    "        input_df = input_df[input_df.x33.notnull()]\n",
    "        input_df = input_df[input_df.x54.notnull()]\n",
    "        data_good = input_df[input_df[\"data_quality\"] == 1]\n",
    "        \n",
    "        ##print(data_good)\n",
    "        return(data_good)\n",
    "    except Exception as error_msg:\n",
    "        logger.info(f\"Exception occured while reading the dataset: {blob_name}\"\n",
    "                    f\"Error Info is  {error_msg}\")\n",
    "\n",
    "\n",
    "def get_response_filters(current_filter_params, df, default_values_selected, all_filters, multi_select_filters, extra_filters={}):\n",
    "    logger.info(\"Preparing filter dictionary\")\n",
    "    # Usage\n",
    "    # -----\n",
    "    # >>> filter_df = pd.DataFrame(columns=[....])    # Optional operation\n",
    "    # >>> filter_df = final_ADS.groupby(......)       # Optional operation\n",
    "    # >>> default_values_selected = {}    # The default value to be selected for a filter, provide filter_name, filter_values\n",
    "    # >>> all_option_filters = []         # Filters with an All option\n",
    "    # >>> multi_select_filters = []       # Filters with an multi_select option\n",
    "    # >>> more_filters = {}               # Extra filters, provide filter_names, filter_options\n",
    "    # >>> final_dict_out = get_response_filters(current_filter_params, filter_df, default_values_selected, all_option_filters, multi_select_filters, more_filters)\n",
    "    # >>> dynamic_outputs = json.dumps(final_dict_out)\n",
    "    # Returns\n",
    "    # -------\n",
    "    # A dict object containing the filters JSON structure\n",
    "\n",
    "    filters = list(df.columns)\n",
    "    default_values_possible = {}\n",
    "    for item in filters:\n",
    "        default_possible = list(df[item].unique())\n",
    "        if item in all_filters:\n",
    "            default_possible = list(chain(['All'], default_possible))\n",
    "        default_values_possible[item] = default_possible\n",
    "    if extra_filters:\n",
    "        filters.extend(list(extra_filters.keys()))\n",
    "        default_values_possible.update(extra_filters)\n",
    "    if current_filter_params:\n",
    "        selected_filters = current_filter_params[\"selected\"]\n",
    "        # current_filter = current_filter_params[\"current_filter\"]\n",
    "        # current_index = filters.index(current_filter)\n",
    "        select_df = df.copy()\n",
    "    final_dict = {}\n",
    "    iter_value = 0\n",
    "    data_values = []\n",
    "    default_values = {}\n",
    "    for item in filters:\n",
    "        filter_dict = {}\n",
    "        filter_dict[\"widget_filter_index\"] = int(iter_value)\n",
    "        filter_dict[\"widget_filter_function\"] = False\n",
    "        filter_dict[\"widget_filter_function_parameter\"] = False\n",
    "        filter_dict[\"widget_filter_hierarchy_key\"] = False\n",
    "        filter_dict[\"widget_filter_isall\"] = True if item in all_filters else False\n",
    "        filter_dict[\"widget_filter_multiselect\"] = True if item in multi_select_filters else False\n",
    "        filter_dict[\"widget_tag_key\"] = str(item)\n",
    "        filter_dict[\"widget_tag_label\"] = str(item)\n",
    "        filter_dict[\"widget_tag_input_type\"] = \"select\",\n",
    "        filter_dict[\"widget_filter_dynamic\"] = True\n",
    "        if current_filter_params:\n",
    "            if item in df.columns:\n",
    "                possible_values = list(select_df[item].unique())\n",
    "                item_default_value = selected_filters[item]\n",
    "                if item in all_filters:\n",
    "                    possible_values = list(chain(['All'], possible_values))\n",
    "                if item in multi_select_filters:\n",
    "                    for value in selected_filters[item]:\n",
    "                        if value not in possible_values:\n",
    "                            if possible_values[0] == \"All\":\n",
    "                                item_default_value = possible_values\n",
    "                            else:\n",
    "                                item_default_value = [possible_values[0]]\n",
    "                else:\n",
    "                    if selected_filters[item] not in possible_values:\n",
    "                        item_default_value = possible_values[0]\n",
    "                filter_dict[\"widget_tag_value\"] = possible_values\n",
    "                if item in multi_select_filters:\n",
    "                    if 'All' not in item_default_value and selected_filters[item]:\n",
    "                        select_df = select_df[select_df[item].isin(\n",
    "                            item_default_value)]\n",
    "                else:\n",
    "                    if selected_filters[item] != 'All':\n",
    "                        select_df = select_df[select_df[item]\n",
    "                                              == item_default_value]\n",
    "            else:\n",
    "                filter_dict[\"widget_tag_value\"] = extra_filters[item]\n",
    "        else:\n",
    "            filter_dict[\"widget_tag_value\"] = default_values_possible[item]\n",
    "            item_default_value = default_values_selected[item]\n",
    "        data_values.append(filter_dict)\n",
    "        default_values[item] = item_default_value\n",
    "        iter_value = iter_value + 1\n",
    "    final_dict[\"dataValues\"] = data_values\n",
    "    final_dict[\"defaultValues\"] = default_values\n",
    "    logger.info(\"Successfully prepared filter dictionary\")\n",
    "    return final_dict\n",
    "\n",
    "\n",
    "def prepare_filter_json():\n",
    "    logger.info(f\"Preparing json for Filters in Historical Screen\")\n",
    "    # Preapre Filter json for Region in the Historical View Screen.\n",
    "    dframe = fiile_read(\"cost-of-living_v2_I0869\")\n",
    "    dframe = dframe.groupby(['country']).sum().reset_index()\n",
    "    filter_dframe = dframe[['country']]\n",
    "    default_values_selected = {\"country\": 'India'}\n",
    "    all_filters = []\n",
    "    multi_select_filters = []\n",
    "    current_filter_params = {\"selected\": default_values_selected}\n",
    "    final_dict_out = get_response_filters(\n",
    "        current_filter_params, filter_dframe, default_values_selected, all_filters, multi_select_filters)\n",
    "    logger.info(f\"Successful prepared json for Filters in Historical Screen\")\n",
    "    return json.dumps(final_dict_out)\n",
    "\n",
    "\n",
    "dynamic_outputs = prepare_filter_json()\n",
    "\"\"\"\n",
    "#END CUSTOM CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BEGIN CUSTOM CODE BELOW...\n",
    "\n",
    "#put your output in this response param for connecting to downstream widgets\n",
    "step_2_Table = \"\"\"\n",
    "# Below codestring is used to display the grid table that consists of historic sales done over time on each product.\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "from io import StringIO\n",
    "\n",
    "\n",
    "def getLogger():\n",
    "    import logging\n",
    "    logging.basicConfig(filename=\"UIACLogger.log\",\n",
    "                        format='%(asctime)s %(message)s',\n",
    "                        filemode='a')\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    return logger\n",
    "\n",
    "\n",
    "logger = getLogger()\n",
    "\n",
    "\n",
    "def fiile_read(blob_name):\n",
    "    #reading the file from azure blob storage container\n",
    "    logger.info(f\"Read dataset file: {blob_name}\")\n",
    "    try:\n",
    "        from azure.storage.blob import BlockBlobService\n",
    "        sas_token = '?sv=2021-04-10&st=2022-12-22T08%3A12%3A47Z&se=2023-12-30T08%3A12%3A00Z&sr=c&sp=racwl&sig=fMeYkXsCvwK%2F0qVrCmj2j3NiMricQjOPWOkAEXekIPA%3D'\n",
    "        account_name = 'willbedeletedsoon'\n",
    "        container_name = 'codx-pede-s02'\n",
    "        #blob_name = 'cost-of-living_v2_I0869'\n",
    "        def get_data_from_blob(sas_token, account_name, container_name, blob_name):\n",
    "            block_blob_service = BlockBlobService(account_name=account_name, sas_token= sas_token)\n",
    "            from_blob = block_blob_service.get_blob_to_text(container_name = container_name, blob_name=blob_name)\n",
    "            return pd.read_csv(StringIO(from_blob.content))\n",
    "        input_df=get_data_from_blob(sas_token, account_name, container_name, blob_name)\n",
    "        #filtering out empty columns\n",
    "        input_df = input_df[input_df.x2.notnull()]\n",
    "        input_df = input_df[input_df.x23.notnull()]\n",
    "        input_df = input_df[input_df.x33.notnull()]\n",
    "        input_df = input_df[input_df.x54.notnull()]\n",
    "        data_good = input_df[input_df[\"data_quality\"] == 1]\n",
    "        \n",
    "        ##print(data_good)\n",
    "        return(input_df)\n",
    "    except Exception as error_msg:\n",
    "        logger.info(f\"Exception occured while reading the dataset: {blob_name}\"\n",
    "                    f\"Error Info is  {error_msg}\")\n",
    "\n",
    "\n",
    "def get_filter_table(dframe, selected_filters):\n",
    "    logger.info(\"Applying screen filters on the grid table dframe.\")\n",
    "    select_df = dframe.copy()\n",
    "    for item in list(selected_filters):\n",
    "        if isinstance(selected_filters[item], list):\n",
    "            if 'All' not in selected_filters[item] and selected_filters[item]:\n",
    "                select_df = select_df[select_df[item].isin(\n",
    "                    selected_filters[item])]\n",
    "        else:\n",
    "            if selected_filters[item] != 'All':\n",
    "                select_df = select_df[select_df[item]\n",
    "                                      == selected_filters[item]]\n",
    "    logger.info(\"Successfully applied screen filters on the grid table dframe.\")\n",
    "    return select_df\n",
    "\n",
    "\n",
    "def generate_dynamic_table(dframe, name='Sales', grid_options={\"tableSize\": \"small\", \"tableMaxHeight\": \"80vh\", \"quickSearch\":True}, group_headers=[], grid=\"auto\"):\n",
    "    logger.info(\"Generate dynamic Grid table json from dframe\")\n",
    "    table_dict = {}\n",
    "    table_props = {}\n",
    "    table_dict.update({\"grid\": grid, \"type\": \"tabularForm\",\n",
    "                      \"noGutterBottom\": True, 'name': name})\n",
    "    values_dict = dframe.dropna(axis=1).to_dict(\"records\")\n",
    "    table_dict.update({\"value\": values_dict})\n",
    "    col_def_list = []\n",
    "    for col in list(dframe.columns):\n",
    "        col_def_dict = {}\n",
    "        col_def_dict.update({\"headerName\": col, \"field\": col})\n",
    "        col_def_list.append(col_def_dict)\n",
    "    table_props[\"groupHeaders\"] = group_headers\n",
    "    table_props[\"coldef\"] = col_def_list\n",
    "    table_props[\"gridOptions\"] = grid_options\n",
    "    table_dict.update({\"tableprops\": table_props})\n",
    "    logger.info(\"Successfully generated dynamic Grid table json from dframe\")\n",
    "    return table_dict\n",
    "\n",
    "\n",
    "def build_grid_table_json():\n",
    "    logger.info(\"Preparing grid table json for Historical Screen\")\n",
    "    form_config = {}\n",
    "    dframe = fiile_read(\"cost-of-living_v2_I0869.csv\")\n",
    "    selected_filters = {\"country\": 'India'}\n",
    "    dframe = get_filter_table(dframe, selected_filters)\n",
    "    dframe1=dframe[['country','x48', 'x37', 'x36', 'x8']]\n",
    "    print(dframe1)\n",
    "    form_config['fields'] = [generate_dynamic_table(dframe1)]\n",
    "    grid_table_json = {}\n",
    "    grid_table_json['form_config'] = form_config\n",
    "    logger.info(\"Successfully prepared grid table json for Historical Screen\")\n",
    "    return grid_table_json\n",
    "\n",
    "\n",
    "grid_table_json = build_grid_table_json()\n",
    "dynamic_outputs = json.dumps(grid_table_json)\n",
    "\n",
    "# \"\"\"\n",
    "# #END CUSTOM CODE done till here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BEGIN CUSTOM CODE BELOW...\n",
    "\n",
    "#put your output in this response param for connecting to downstream widgets\n",
    "step_3_Table = \"\"\"\n",
    "# Below codestring is used to display the grid table that consists of historic sales done over time on each product.\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "from io import StringIO\n",
    "\n",
    "\n",
    "def getLogger():\n",
    "    import logging\n",
    "    logging.basicConfig(filename=\"UIACLogger.log\",\n",
    "                        format='%(asctime)s %(message)s',\n",
    "                        filemode='a')\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    return logger\n",
    "\n",
    "\n",
    "logger = getLogger()\n",
    "\n",
    "\n",
    "def file_read(blob_name):\n",
    "    #reading the file from azure blob storage container\n",
    "    logger.info(f\"Read dataset file: {blob_name}\")\n",
    "    try:\n",
    "        from azure.storage.blob import BlockBlobService\n",
    "        sas_token = '?sv=2021-04-10&st=2022-12-22T08%3A12%3A47Z&se=2023-12-30T08%3A12%3A00Z&sr=c&sp=racwl&sig=fMeYkXsCvwK%2F0qVrCmj2j3NiMricQjOPWOkAEXekIPA%3D'\n",
    "        account_name = 'willbedeletedsoon'\n",
    "        container_name = 'codx-pede-s02'\n",
    "        #blob_name = 'cost-of-living_v2_I0869' fiile_read(\"cost-of-living_v2_I0869\")\n",
    "        def get_data_from_blob(sas_token, account_name, container_name, blob_name):\n",
    "            block_blob_service = BlockBlobService(account_name=account_name, sas_token= sas_token)\n",
    "            from_blob = block_blob_service.get_blob_to_text(container_name = container_name, blob_name=blob_name)\n",
    "            return pd.read_csv(StringIO(from_blob.content))\n",
    "        input_df=get_data_from_blob(sas_token, account_name, container_name, blob_name)\n",
    "        #filtering out empty columns\n",
    "        input_df = input_df[input_df.x2.notnull()]\n",
    "        input_df = input_df[input_df.x8.notnull()]\n",
    "        input_df = input_df[input_df.x33.notnull()]\n",
    "        input_df = input_df[input_df.x54.notnull()]\n",
    "        input_df = input_df[input_df.x48.notnull()]\n",
    "        input_df = input_df[input_df.x37.notnull()]\n",
    "        input_df = input_df[input_df.x36.notnull()]\n",
    "        data_good = input_df[input_df[\"data_quality\"] == 1]\n",
    "        \n",
    "        #print(data_good)\n",
    "        return(data_good)\n",
    "    except Exception as error_msg:\n",
    "        logger.info(f\"Exception occured while reading the dataset: {blob_name}\"\n",
    "                    f\"Error Info is  {error_msg}\")\n",
    "\n",
    "\n",
    "def get_filter_table(dframe, selected_filters):\n",
    "    logger.info(\"Applying screen filters on the grid table dframe.\")\n",
    "    select_df = dframe.copy()\n",
    "    for item in list(selected_filters):\n",
    "        if isinstance(selected_filters[item], list):\n",
    "            if 'All' not in selected_filters[item] and selected_filters[item]:\n",
    "                select_df = select_df[select_df[item].isin(\n",
    "                    selected_filters[item])]\n",
    "        else:\n",
    "            if selected_filters[item] != 'All':\n",
    "                select_df = select_df[select_df[item]\n",
    "                                      == selected_filters[item]]\n",
    "    logger.info(\"Successfully applied screen filters on the grid table dframe.\")\n",
    "    return select_df\n",
    "\n",
    "\n",
    "def generate_dynamic_table(dframe, name='Sales', grid_options={\"tableSize\": \"small\", \"tableMaxHeight\": \"80vh\", \"quickSearch\":True}, group_headers=[], grid=\"auto\"):\n",
    "    logger.info(\"Generate dynamic Grid table json from dframe\")\n",
    "    table_dict = {}\n",
    "    table_props = {}\n",
    "    table_dict.update({\"grid\": grid, \"type\": \"tabularForm\",\n",
    "                      \"noGutterBottom\": True, 'name': name})\n",
    "    values_dict = dframe.dropna(axis=1).to_dict(\"records\")\n",
    "    table_dict.update({\"value\": values_dict})\n",
    "    col_def_list = []\n",
    "    for col in list(dframe.columns):\n",
    "        col_def_dict = {}\n",
    "        col_def_dict.update({\"headerName\": col, \"field\": col})\n",
    "        col_def_list.append(col_def_dict)\n",
    "    table_props[\"groupHeaders\"] = group_headers\n",
    "    table_props[\"coldef\"] = col_def_list\n",
    "    table_props[\"gridOptions\"] = grid_options\n",
    "    table_dict.update({\"tableprops\": table_props})\n",
    "    logger.info(\"Successfully generated dynamic Grid table json from dframe\")\n",
    "    return table_dict\n",
    "\n",
    "\n",
    "def build_grid_table_json():\n",
    "    logger.info(\"Preparing grid table json for Historical Screen\")\n",
    "    form_config = {}\n",
    "    dframe = file_read(\"cost-of-living_v2_I0869.csv\")\n",
    "    #print(dframe)\n",
    "    selected_filters = {\"country\": 'India'}\n",
    "    #print(dframe)\n",
    "    dframe1=dframe[['country','x48', 'x37', 'x36', 'x8']] #--here281222 --5 columns\n",
    "    print(dframe1)\n",
    "    dframe = get_filter_table(dframe, selected_filters)\n",
    "    form_config['fields'] = [generate_dynamic_table(dframe)]\n",
    "    grid_table_json = {}\n",
    "    grid_table_json['form_config'] = form_config\n",
    "    logger.info(\"Successfully prepared grid table json for Historical Screen\")\n",
    "    return grid_table_json\n",
    "\n",
    "\n",
    "grid_table_json = build_grid_table_json()\n",
    "dynamic_outputs = json.dumps(grid_table_json)\n",
    "\n",
    "# \"\"\"\n",
    "# #END CUSTOM CODE done till here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BEGIN CUSTOM CODE BELOW...\n",
    "\n",
    "#put your output in this response param for connecting to downstream widgets\n",
    "step_4 = \"\"\"\n",
    "# Below codestring is used to perform detailed analysis on quantity of sales done over time. \n",
    "\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import json\n",
    "from io import StringIO\n",
    "import plotly.io as io\n",
    "\n",
    "\n",
    "def getLogger():\n",
    "    import logging\n",
    "    logging.basicConfig(filename=\"UIACLogger.log\",\n",
    "                        format='%(asctime)s %(message)s',\n",
    "                        filemode='a')\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    return logger\n",
    "\n",
    "\n",
    "logger = getLogger()\n",
    "\n",
    "\n",
    "def fiile_read(blob_name):\n",
    "    #reading the file from azure blob storage container\n",
    "    logger.info(f\"Read dataset file: {blob_name}\")\n",
    "    try:\n",
    "        from azure.storage.blob import BlockBlobService\n",
    "        sas_token = '?sv=2021-04-10&st=2022-12-22T08%3A12%3A47Z&se=2023-12-30T08%3A12%3A00Z&sr=c&sp=racwl&sig=fMeYkXsCvwK%2F0qVrCmj2j3NiMricQjOPWOkAEXekIPA%3D'\n",
    "        account_name = 'willbedeletedsoon'\n",
    "        container_name = 'codx-pede-s02'\n",
    "        #blob_name = 'cost-of-living_v2_I0869'\n",
    "        def get_data_from_blob(sas_token, account_name, container_name, blob_name):\n",
    "            block_blob_service = BlockBlobService(account_name=account_name, sas_token= sas_token)\n",
    "            from_blob = block_blob_service.get_blob_to_text(container_name = container_name, blob_name=blob_name)\n",
    "            return pd.read_csv(StringIO(from_blob.content))\n",
    "        input_df=get_data_from_blob(sas_token, account_name, container_name, blob_name)\n",
    "        #filtering out empty columns\n",
    "        input_df = input_df[input_df.x2.notnull()]\n",
    "        input_df = input_df[input_df.x23.notnull()]\n",
    "        input_df = input_df[input_df.x33.notnull()]\n",
    "        input_df = input_df[input_df.x54.notnull()]\n",
    "        data_good = input_df[input_df[\"data_quality\"] == 1]\n",
    "        \n",
    "        ##print(data_good)\n",
    "        return(input_df)\n",
    "    except Exception as error_msg:\n",
    "        logger.info(f\"Exception occured while reading the dataset: {blob_name}\"\n",
    "                    f\"Error Info is  {error_msg}\")\n",
    "\n",
    "\n",
    "def getGraph(dframe, filters):\n",
    "    logger.info(\n",
    "        \"Preparing bar graph json to understand basic_expenditure across all countries\")\n",
    "    column_names = ['x48', 'x38', 'x37', 'x36', 'x33', 'x8', 'x29']\n",
    "    #print(dframe['country'])\n",
    "    data_good = dframe[dframe[\"data_quality\"] == 1]\n",
    "    data_good['basic_expenditure']= data_good[column_names].sum(axis=1)\n",
    "    for item in filters:\n",
    "        if 'All' in filters[item]:\n",
    "            continue\n",
    "        elif isinstance(filters[item], list):\n",
    "            data_good = data_good[data_good[item].isin(filters[item])]\n",
    "        else:\n",
    "            data_good = data_good[data_good[item] == filters[item]]\n",
    "    fig = px.bar(data_good, x='basic_expenditure', y='country') #, color='city'\n",
    "    #fig = px.scatter(data_good, y='basic_expenditure', x='country')\n",
    "    #print(data_good[['basic_expenditure','country']].where(data_good['country'] == 'Denmark')) #--here3\n",
    "    #data_good1 = data_good[data_good['country'] == 'Denmark']\n",
    "    #print(data_good1)\n",
    "    fig.show()\n",
    "    logger.info(\n",
    "        \"Successfully prepared bar graph json to understand basic_expenditure across all countries\")\n",
    "    return io.to_json(fig)\n",
    "\n",
    "\n",
    "selected_filters = {\"country\": ['India','United States']}\n",
    "dframe = fiile_read(\"cost-of-living_v2_I0869.csv\")\n",
    "#dframe = dframe.groupby(['country'])\n",
    "dynamic_outputs = getGraph(dframe, selected_filters)#here11\n",
    "\n",
    "#\"\"\"\n",
    "# #END CUSTOM CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_4_a =\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import json\n",
    "from io import StringIO\n",
    "import plotly.io as io\n",
    "\n",
    "\n",
    "def getLogger():\n",
    "    import logging\n",
    "    logging.basicConfig(filename=\"UIACLogger.log\",\n",
    "                        format='%(asctime)s %(message)s',\n",
    "                        filemode='a')\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    return logger\n",
    "\n",
    "\n",
    "logger = getLogger()\n",
    "\n",
    "\n",
    "\n",
    "def getGraph(dframe, filters):\n",
    "    logger.info(\n",
    "        \"Preparing scatter plot json to understand relation between x and y values\")\n",
    "df = pd.DataFrame(np.random.randint(0,100,size=(100, 1)), columns=list('X'))\n",
    "df = df.assign(Y = lambda x: (x['X']**2))\n",
    "fig = px.scatter(df, y='Y', x='X')\n",
    "fig.show()\n",
    "#print(df)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_5 =\"\"\"\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import json\n",
    "from io import StringIO\n",
    "import plotly.io as io\n",
    "\n",
    "\n",
    "def getLogger():\n",
    "    import logging\n",
    "    logging.basicConfig(filename=\"UIACLogger.log\",\n",
    "                        format='%(asctime)s %(message)s',\n",
    "                        filemode='a')\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    return logger\n",
    "\n",
    "\n",
    "logger = getLogger()\n",
    "\n",
    "def fiile_read(blob_name):\n",
    "    #reading the file from azure blob storage container\n",
    "    logger.info(f\"Read dataset file: {blob_name}\")\n",
    "    try:\n",
    "        from azure.storage.blob import BlockBlobService\n",
    "        sas_token = '?sv=2021-04-10&st=2022-12-22T08%3A12%3A47Z&se=2023-12-30T08%3A12%3A00Z&sr=c&sp=racwl&sig=fMeYkXsCvwK%2F0qVrCmj2j3NiMricQjOPWOkAEXekIPA%3D'\n",
    "        account_name = 'willbedeletedsoon'\n",
    "        container_name = 'codx-pede-s02'\n",
    "        #blob_name = 'cost-of-living_v2_I0869'\n",
    "        def get_data_from_blob(sas_token, account_name, container_name, blob_name):\n",
    "            block_blob_service = BlockBlobService(account_name=account_name, sas_token= sas_token)\n",
    "            from_blob = block_blob_service.get_blob_to_text(container_name = container_name, blob_name=blob_name)\n",
    "            return pd.read_csv(StringIO(from_blob.content))\n",
    "        input_df=get_data_from_blob(sas_token, account_name, container_name, blob_name)\n",
    "        #filtering out empty columns\n",
    "        input_df = input_df[input_df.x2.notnull()]\n",
    "        input_df = input_df[input_df.x23.notnull()]\n",
    "        input_df = input_df[input_df.x33.notnull()]\n",
    "        input_df = input_df[input_df.x54.notnull()]\n",
    "        data_good = input_df[input_df[\"data_quality\"] == 1]\n",
    "        \n",
    "        ##print(data_good)\n",
    "        return(input_df)\n",
    "    except Exception as error_msg:\n",
    "        logger.info(f\"Exception occured while reading the dataset: {blob_name}\"\n",
    "                    f\"Error Info is  {error_msg}\")\n",
    "\n",
    "def getGraph():\n",
    "    logger.info(\n",
    "        \"Preparing scatter plot json to understand relation between x and y values\")\n",
    "    # load dataset\n",
    "    #df = pd.DataFrame(np.random.randint(0,100,size=(100, 2)), columns=list('XY'))\n",
    "    #df = df.assign(Y = lambda x: (x['X']**2))\n",
    "    #df = pd.read_csv(\"https://raw.githubusercontent.com/plotly/datasets/master/volcano.csv\")\n",
    "    input_df=fiile_read(\"cost-of-living_v2_I0869.csv\")\n",
    "        #filtering out empty columns\n",
    "    df = input_df[input_df[\"data_quality\"] == 1].groupby(['city'])\n",
    "    df = input_df[input_df[\"country\"] == 'India']\n",
    "    #print(df)\n",
    "    # create figure\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add surface trace\n",
    "    fig.add_trace(go.Surface(  x=df.city,y=df.x54, colorscale=\"Viridis\"))\n",
    "\n",
    "    # Update plot sizing\n",
    "    # fig.update_layout(\n",
    "    #     width=800,\n",
    "    #     height=900,\n",
    "    #     autosize=False,\n",
    "    #     margin=dict(t=0, b=0, l=0, r=0),\n",
    "    #     template=\"plotly_white\",\n",
    "    # )\n",
    "\n",
    "    # Update 3D scene options\n",
    "    fig.update_scenes(\n",
    "        aspectratio=dict(x=1, y=1, z=0.7),\n",
    "        aspectmode=\"manual\"\n",
    "    )\n",
    "\n",
    "    # Add dropdown\n",
    "    fig.update_layout(\n",
    "        updatemenus=[\n",
    "            dict(\n",
    "                buttons=list([\n",
    "                    dict(\n",
    "                        args=[\"type\", \"bar\"],\n",
    "                        label=\"bar\",\n",
    "                        method=\"restyle\"\n",
    "                    ),\n",
    "                    dict(\n",
    "                        args=[\"type\", \"scatter\"],\n",
    "                        label=\"scatter\",\n",
    "                        method=\"restyle\"\n",
    "                    ),\n",
    "                    dict(\n",
    "                        args=[\"type\", \"violin\"],\n",
    "                        label=\"violin\",\n",
    "                        method=\"restyle\"\n",
    "                    ),\n",
    "                    dict(\n",
    "                        args=[\"type\", \"column\"],\n",
    "                        label=\"column\",\n",
    "                        method=\"restyle\"\n",
    "                    ),\n",
    "                    dict(\n",
    "                        args=[\"type\", \"box\"],\n",
    "                        label=\"box\",\n",
    "                        method=\"restyle\"\n",
    "                    )\n",
    "                ]),\n",
    "                direction=\"down\",\n",
    "                pad={\"r\": 10, \"t\": 10},\n",
    "                showactive=True,\n",
    "                x=0.1,\n",
    "                xanchor=\"left\",\n",
    "                y=1.1,\n",
    "                yanchor=\"top\"\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Add annotation\n",
    "    fig.update_layout(\n",
    "        annotations=[\n",
    "            dict(text=\"Trace type:\", showarrow=False,\n",
    "            x=0, y=1.085, yref=\"paper\", align=\"left\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "dynamic_outputs = getGraph()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots_dict={\n",
    "    \"step_2_Table\":step_2_Table,\n",
    "    \"step_3_Table\":step_3_Table,\n",
    "    \"step_1\":step_1,\n",
    "    \"step_4_a\":step_4_a,\n",
    "    \"step_5\":step_5\n",
    "}\n",
    "results_json.append({\n",
    "    'type':'review',\n",
    "    'name': 'overview',\n",
    "    'component':'overview',\n",
    "    'dynamic_visual_results': plots_dict      \n",
    "})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "currentNotebook = 'Downloads\\depe1.ipynb'\n",
    "\n",
    "!jupyter nbconvert --to script {currentNotebook} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "utils.submit_config_params(url='https://codex-api-stage.azurewebsites.net/codex-api/projects/upload-config-params/XbpsYR9JylEWw7cfZo3YLw', nb_name=currentNotebook, results=results_json, codex_tags=codex_tags, args={})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a96afb4c5e530be632fe4ae220902658aaf598ffb07f39eceab225b5ab973af6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
