{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tags to identify this iteration when submitted\n",
    "# example: codex_tags = {'env': 'dev', 'region': 'USA', 'product_category': 'A'}\n",
    "\n",
    "codex_tags = {\n",
    "}\n",
    "\n",
    "from codex_widget_factory import utils\n",
    "results_json=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BEGIN CUSTOM CODE BELOW...\n",
    "\n",
    "#put your output in this response param for connecting to downstream widgets\n",
    "HistoricScreenFilters = \"\"\"\n",
    "## Below codestring is used to create filters to show different regions data in  product historic screen.\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "def getLogger():\n",
    "    import logging\n",
    "    logging.basicConfig(filename=\"UIACLogger.log\",\n",
    "                        format='%(asctime)s %(message)s',\n",
    "                        filemode='a')\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    return logger\n",
    "\n",
    "\n",
    "logger = getLogger()\n",
    "\n",
    "\n",
    "def fiile_read(blob_name):\n",
    "    #reading the file from azure blob storage container\n",
    "    logger.info(f\"Read dataset file: {blob_name}\")\n",
    "    try:\n",
    "        from azure.storage.blob import BlockBlobService\n",
    "        sas_token = '?sv=2021-04-10&st=2022-12-22T08%3A12%3A47Z&se=2023-12-30T08%3A12%3A00Z&sr=c&sp=racwl&sig=fMeYkXsCvwK%2F0qVrCmj2j3NiMricQjOPWOkAEXekIPA%3D'\n",
    "        account_name = 'willbedeletedsoon'\n",
    "        container_name = 'codx-pede-s02'\n",
    "        #blob_name = 'GlobalLandTemperaturesByCity_I0869'\n",
    "        def get_data_from_blob(sas_token, account_name, container_name, blob_name):\n",
    "            block_blob_service = BlockBlobService(account_name=account_name, sas_token= sas_token)\n",
    "            from_blob = block_blob_service.get_blob_to_text(container_name = container_name, blob_name=blob_name)\n",
    "            return pd.read_csv(StringIO(from_blob.content))\n",
    "        input_df=get_data_from_blob(sas_token, account_name, container_name, blob_name)\n",
    "        #filtering out empty columns\n",
    "        input_df = input_df[input_df.x2.notnull()]\n",
    "        input_df = input_df[input_df.x23.notnull()]\n",
    "        input_df = input_df[input_df.x33.notnull()]\n",
    "        input_df = input_df[input_df.x54.notnull()]\n",
    "        \n",
    "        #print(input_df)\n",
    "        return(input_df)\n",
    "    except Exception as error_msg:\n",
    "        logger.info(f\"Exception occured while reading the dataset: {blob_name}\"\n",
    "                    f\"Error Info is  {error_msg}\")\n",
    "\n",
    "\n",
    "def get_response_filters(current_filter_params, df, default_values_selected, all_filters, multi_select_filters, extra_filters={}):\n",
    "    logger.info(\"Preparing filter dictionary\")\n",
    "    # Usage\n",
    "    # -----\n",
    "    # >>> filter_df = pd.DataFrame(columns=[....])    # Optional operation\n",
    "    # >>> filter_df = final_ADS.groupby(......)       # Optional operation\n",
    "    # >>> default_values_selected = {}    # The default value to be selected for a filter, provide filter_name, filter_values\n",
    "    # >>> all_option_filters = []         # Filters with an All option\n",
    "    # >>> multi_select_filters = []       # Filters with an multi_select option\n",
    "    # >>> more_filters = {}               # Extra filters, provide filter_names, filter_options\n",
    "    # >>> final_dict_out = get_response_filters(current_filter_params, filter_df, default_values_selected, all_option_filters, multi_select_filters, more_filters)\n",
    "    # >>> dynamic_outputs = json.dumps(final_dict_out)\n",
    "    # Returns\n",
    "    # -------\n",
    "    # A dict object containing the filters JSON structure\n",
    "\n",
    "    filters = list(df.columns)\n",
    "    default_values_possible = {}\n",
    "    for item in filters:\n",
    "        default_possible = list(df[item].unique())\n",
    "        if item in all_filters:\n",
    "            default_possible = list(chain(['All'], default_possible))\n",
    "        default_values_possible[item] = default_possible\n",
    "    if extra_filters:\n",
    "        filters.extend(list(extra_filters.keys()))\n",
    "        default_values_possible.update(extra_filters)\n",
    "    if current_filter_params:\n",
    "        selected_filters = current_filter_params[\"selected\"]\n",
    "        # current_filter = current_filter_params[\"current_filter\"]\n",
    "        # current_index = filters.index(current_filter)\n",
    "        select_df = df.copy()\n",
    "    final_dict = {}\n",
    "    iter_value = 0\n",
    "    data_values = []\n",
    "    default_values = {}\n",
    "    for item in filters:\n",
    "        filter_dict = {}\n",
    "        filter_dict[\"widget_filter_index\"] = int(iter_value)\n",
    "        filter_dict[\"widget_filter_function\"] = False\n",
    "        filter_dict[\"widget_filter_function_parameter\"] = False\n",
    "        filter_dict[\"widget_filter_hierarchy_key\"] = False\n",
    "        filter_dict[\"widget_filter_isall\"] = True if item in all_filters else False\n",
    "        filter_dict[\"widget_filter_multiselect\"] = True if item in multi_select_filters else False\n",
    "        filter_dict[\"widget_tag_key\"] = str(item)\n",
    "        filter_dict[\"widget_tag_label\"] = str(item)\n",
    "        filter_dict[\"widget_tag_input_type\"] = \"select\",\n",
    "        filter_dict[\"widget_filter_dynamic\"] = True\n",
    "        if current_filter_params:\n",
    "            if item in df.columns:\n",
    "                possible_values = list(select_df[item].unique())\n",
    "                item_default_value = selected_filters[item]\n",
    "                if item in all_filters:\n",
    "                    possible_values = list(chain(['All'], possible_values))\n",
    "                if item in multi_select_filters:\n",
    "                    for value in selected_filters[item]:\n",
    "                        if value not in possible_values:\n",
    "                            if possible_values[0] == \"All\":\n",
    "                                item_default_value = possible_values\n",
    "                            else:\n",
    "                                item_default_value = [possible_values[0]]\n",
    "                else:\n",
    "                    if selected_filters[item] not in possible_values:\n",
    "                        item_default_value = possible_values[0]\n",
    "                filter_dict[\"widget_tag_value\"] = possible_values\n",
    "                if item in multi_select_filters:\n",
    "                    if 'All' not in item_default_value and selected_filters[item]:\n",
    "                        select_df = select_df[select_df[item].isin(\n",
    "                            item_default_value)]\n",
    "                else:\n",
    "                    if selected_filters[item] != 'All':\n",
    "                        select_df = select_df[select_df[item]\n",
    "                                              == item_default_value]\n",
    "            else:\n",
    "                filter_dict[\"widget_tag_value\"] = extra_filters[item]\n",
    "        else:\n",
    "            filter_dict[\"widget_tag_value\"] = default_values_possible[item]\n",
    "            item_default_value = default_values_selected[item]\n",
    "        data_values.append(filter_dict)\n",
    "        default_values[item] = item_default_value\n",
    "        iter_value = iter_value + 1\n",
    "    final_dict[\"dataValues\"] = data_values\n",
    "    final_dict[\"defaultValues\"] = default_values\n",
    "    logger.info(\"Successfully prepared filter dictionary\")\n",
    "    return final_dict\n",
    "\n",
    "\n",
    "def prepare_filter_json():\n",
    "    logger.info(f\"Preparing json for Filters in Historical Screen\")\n",
    "    # Preapre Filter json for Region in the Historical View Screen.\n",
    "    dframe = fiile_read(\"cost-of-living_v2_I0869\")\n",
    "    dframe = dframe.groupby(['country']).sum().reset_index()\n",
    "    filter_dframe = dframe[['country']]\n",
    "    default_values_selected = {\"country\": 'India'}\n",
    "    all_filters = []\n",
    "    multi_select_filters = []\n",
    "    current_filter_params = {\"selected\": default_values_selected}\n",
    "    final_dict_out = get_response_filters(\n",
    "        current_filter_params, filter_dframe, default_values_selected, all_filters, multi_select_filters)\n",
    "    logger.info(f\"Successful prepared json for Filters in Historical Screen\")\n",
    "    return json.dumps(final_dict_out)\n",
    "\n",
    "\n",
    "dynamic_outputs = prepare_filter_json()\n",
    "\"\"\"\n",
    "#END CUSTOM CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BEGIN CUSTOM CODE BELOW...\n",
    "\n",
    "#put your output in this response param for connecting to downstream widgets\n",
    "historicScreenGridTable = \"\"\"\n",
    "# Below codestring is used to display the grid table that consists of historic sales done over time on each product.\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "def getLogger():\n",
    "    import logging\n",
    "    logging.basicConfig(filename=\"UIACLogger.log\",\n",
    "                        format='%(asctime)s %(message)s',\n",
    "                        filemode='a')\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    return logger\n",
    "\n",
    "\n",
    "logger = getLogger()\n",
    "\n",
    "\n",
    "def fiile_read(blob_name):\n",
    "    #reading the file from azure blob storage container\n",
    "    logger.info(f\"Read dataset file: {blob_name}\")\n",
    "    try:\n",
    "        from azure.storage.blob import BlockBlobService\n",
    "        sas_token = '?sv=2021-04-10&st=2022-12-22T08%3A12%3A47Z&se=2023-12-30T08%3A12%3A00Z&sr=c&sp=racwl&sig=fMeYkXsCvwK%2F0qVrCmj2j3NiMricQjOPWOkAEXekIPA%3D'\n",
    "        account_name = 'willbedeletedsoon'\n",
    "        container_name = 'codx-pede-s02'\n",
    "        #blob_name = 'GlobalLandTemperaturesByCity_I0869'\n",
    "        def get_data_from_blob(sas_token, account_name, container_name, blob_name):\n",
    "            block_blob_service = BlockBlobService(account_name=account_name, sas_token= sas_token)\n",
    "            from_blob = block_blob_service.get_blob_to_text(container_name = container_name, blob_name=blob_name)\n",
    "            return pd.read_csv(StringIO(from_blob.content))\n",
    "        input_df=get_data_from_blob(sas_token, account_name, container_name, blob_name)\n",
    "        #filtering out empty columns\n",
    "        input_df = input_df[input_df.x2.notnull()]\n",
    "        input_df = input_df[input_df.x23.notnull()]\n",
    "        input_df = input_df[input_df.x33.notnull()]\n",
    "        input_df = input_df[input_df.x54.notnull()]\n",
    "        \n",
    "        #print(input_df)\n",
    "        return(input_df)\n",
    "    except Exception as error_msg:\n",
    "        logger.info(f\"Exception occured while reading the dataset: {blob_name}\"\n",
    "                    f\"Error Info is  {error_msg}\")\n",
    "\n",
    "\n",
    "def get_filter_table(dframe, selected_filters):\n",
    "    logger.info(\"Applying screen filters on the grid table dframe.\")\n",
    "    select_df = dframe.copy()\n",
    "    for item in list(selected_filters):\n",
    "        if isinstance(selected_filters[item], list):\n",
    "            if 'All' not in selected_filters[item] and selected_filters[item]:\n",
    "                select_df = select_df[select_df[item].isin(\n",
    "                    selected_filters[item])]\n",
    "        else:\n",
    "            if selected_filters[item] != 'All':\n",
    "                select_df = select_df[select_df[item]\n",
    "                                      == selected_filters[item]]\n",
    "    logger.info(\"Successfully applied screen filters on the grid table dframe.\")\n",
    "    return select_df\n",
    "\n",
    "\n",
    "def generate_dynamic_table(dframe, name='Sales', grid_options={\"tableSize\": \"small\", \"tableMaxHeight\": \"80vh\", \"quickSearch\":True}, group_headers=[], grid=\"auto\"):\n",
    "    logger.info(\"Generate dynamic Grid table json from dframe\")\n",
    "    table_dict = {}\n",
    "    table_props = {}\n",
    "    table_dict.update({\"grid\": grid, \"type\": \"tabularForm\",\n",
    "                      \"noGutterBottom\": True, 'name': name})\n",
    "    values_dict = dframe.dropna(axis=1).to_dict(\"records\")\n",
    "    table_dict.update({\"value\": values_dict})\n",
    "    col_def_list = []\n",
    "    for col in list(dframe.columns):\n",
    "        col_def_dict = {}\n",
    "        col_def_dict.update({\"headerName\": col, \"field\": col})\n",
    "        col_def_list.append(col_def_dict)\n",
    "    table_props[\"groupHeaders\"] = group_headers\n",
    "    table_props[\"coldef\"] = col_def_list\n",
    "    table_props[\"gridOptions\"] = grid_options\n",
    "    table_dict.update({\"tableprops\": table_props})\n",
    "    logger.info(\"Successfully generated dynamic Grid table json from dframe\")\n",
    "    return table_dict\n",
    "\n",
    "\n",
    "def build_grid_table_json():\n",
    "    logger.info(\"Preparing grid table json for Historical Screen\")\n",
    "    form_config = {}\n",
    "    dframe = fiile_read(\"cost-of-living_v2_I0869\")\n",
    "    selected_filters = {\"country\": 'India'}\n",
    "    dframe = get_filter_table(dframe, selected_filters)\n",
    "    form_config['fields'] = [generate_dynamic_table(dframe)]\n",
    "    grid_table_json = {}\n",
    "    grid_table_json['form_config'] = form_config\n",
    "    logger.info(\"Successfully prepared grid table json for Historical Screen\")\n",
    "    return grid_table_json\n",
    "\n",
    "\n",
    "grid_table_json = build_grid_table_json()\n",
    "dynamic_outputs = json.dumps(grid_table_json)\n",
    "\n",
    "\"\"\"\n",
    "#END CUSTOM CODE done till here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEGIN CUSTOM CODE BELOW...\n",
    "\n",
    "#put your output in this response param for connecting to downstream widgets\n",
    "historicScreenActionGen = \"\"\"\n",
    "# Below codestring is used to create action generator that triggers action handler  to display breadcrumbs \n",
    "# to show total units of product sale over time.\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "def getLogger():\n",
    "    import logging\n",
    "    logging.basicConfig(filename=\"UIACLogger.log\",\n",
    "                        format='%(asctime)s %(message)s',\n",
    "                        filemode='a')\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    return logger\n",
    "\n",
    "\n",
    "logger = getLogger()\n",
    "\n",
    "\n",
    "def get_action_generator():\n",
    "    logger.info(f\"Preparing action generator json for Historical screen\")\n",
    "    action_generator = {\n",
    "        \"actions\": [{\n",
    "            \"action_type\": \"get_screen_breadcrumbs\",\n",
    "            \"component_type\": \"text_list\",\n",
    "            \"params\": {\n",
    "                \"fetch_on_load\": True\n",
    "            },\n",
    "            \"position\": {\n",
    "                \"portal\": \"screen_top_left\"\n",
    "            }\n",
    "        }]\n",
    "    }\n",
    "\n",
    "    logger.info(\n",
    "        f\"Successfully prepared action generator json for Historical screen\")\n",
    "    return action_generator\n",
    "\n",
    "\n",
    "res = get_action_generator()\n",
    "dynamic_outputs = json.dumps(res)\n",
    "\n",
    "\"\"\"\n",
    "#END CUSTOM CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BEGIN CUSTOM CODE BELOW...\n",
    "\n",
    "#put your output in this response param for connecting to downstream widgets\n",
    "\n",
    "#historicScreenActionHandler = \"\"\"\n",
    "# Below codestring is used to generate json for breadcrumbs to display total number units of sales done over time.\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "def getLogger():\n",
    "    import logging\n",
    "    logging.basicConfig(filename=\"UIACLogger.log\",\n",
    "                        format='%(asctime)s %(message)s',\n",
    "                        filemode='a')\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    return logger\n",
    "\n",
    "\n",
    "logger = getLogger()\n",
    "\n",
    "\n",
    "def fiile_read(blob_name):\n",
    "    #reading the file from azure blob storage container\n",
    "    logger.info(f\"Read dataset file: {blob_name}\")\n",
    "    try:\n",
    "        from azure.storage.blob import BlockBlobService\n",
    "        sas_token = '?sv=2021-04-10&st=2022-12-22T08%3A12%3A47Z&se=2023-12-30T08%3A12%3A00Z&sr=c&sp=racwl&sig=fMeYkXsCvwK%2F0qVrCmj2j3NiMricQjOPWOkAEXekIPA%3D'\n",
    "        account_name = 'willbedeletedsoon'\n",
    "        container_name = 'codx-pede-s02'\n",
    "        #blob_name = 'GlobalLandTemperaturesByCity_I0869'\n",
    "        def get_data_from_blob(sas_token, account_name, container_name, blob_name):\n",
    "            block_blob_service = BlockBlobService(account_name=account_name, sas_token= sas_token)\n",
    "            from_blob = block_blob_service.get_blob_to_text(container_name = container_name, blob_name=blob_name)\n",
    "            return pd.read_csv(StringIO(from_blob.content))\n",
    "        input_df=get_data_from_blob(sas_token, account_name, container_name, blob_name)\n",
    "        #filtering out empty columns\n",
    "        input_df = input_df[input_df.x2.notnull()]\n",
    "        input_df = input_df[input_df.x23.notnull()]\n",
    "        input_df = input_df[input_df.x33.notnull()]\n",
    "        input_df = input_df[input_df.x54.notnull()]\n",
    "        \n",
    "        #print(input_df)\n",
    "        return(input_df)\n",
    "    except Exception as error_msg:\n",
    "        logger.info(f\"Exception occured while reading the dataset: {blob_name}\"\n",
    "                    f\"Error Info is  {error_msg}\")\n",
    "\n",
    "\n",
    "def get_quantity_breadcrumbs():\n",
    "    logger.info(\n",
    "        f\"Preparing action handler json for order quantity in historical screen\")\n",
    "    dframe = fiile_read(\"cost-of-living_v2_I0869\")\n",
    "    total_quantity = dframe['OrderQuantity'].sum()\n",
    "    actions = {\n",
    "        \"list\": [\n",
    "            {\n",
    "                \"text\": \"number of people: \",\n",
    "            },\n",
    "            {\n",
    "                \"text\": str(total_quantity),\n",
    "                \"color\": \"contrast\",\n",
    "                \"style\": {\n",
    "                    \"fontWeight\": 600,\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    logger.info(\n",
    "        f\"Successfully prepared action handler json for order quantity in historical screen\")\n",
    "    return actions\n",
    "\n",
    "\n",
    "# action_type = \"get_screen_breadcrumbs\"\n",
    "\n",
    "\n",
    "if action_type == \"get_screen_breadcrumbs\":\n",
    "    res = get_quantity_breadcrumbs()\n",
    "\n",
    "dynamic_outputs = json.dumps(res)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#END CUSTOM CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BEGIN CUSTOM CODE BELOW...\n",
    "\n",
    "#put your output in this response param for connecting to downstream widgets\n",
    "#productReturn_quantity = \"\"\"\n",
    "# Below codestring is used to display the nuumber of product unit returned in a year.\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "def getLogger():\n",
    "    import logging\n",
    "    logging.basicConfig(filename=\"UIACLogger.log\",\n",
    "                        format='%(asctime)s %(message)s',\n",
    "                        filemode='a')\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    return logger\n",
    "\n",
    "\n",
    "logger = getLogger()\n",
    "\n",
    "\n",
    "def fiile_read(blob_name):\n",
    "    #reading the file from azure blob storage container\n",
    "    logger.info(f\"Read dataset file: {blob_name}\")\n",
    "    try:\n",
    "        from azure.storage.blob import BlockBlobService\n",
    "        sas_token = '?sv=2021-04-10&st=2022-12-22T08%3A12%3A47Z&se=2023-12-30T08%3A12%3A00Z&sr=c&sp=racwl&sig=fMeYkXsCvwK%2F0qVrCmj2j3NiMricQjOPWOkAEXekIPA%3D'\n",
    "        account_name = 'willbedeletedsoon'\n",
    "        container_name = 'codx-pede-s02'\n",
    "        #blob_name = 'GlobalLandTemperaturesByCity_I0869'\n",
    "        def get_data_from_blob(sas_token, account_name, container_name, blob_name):\n",
    "            block_blob_service = BlockBlobService(account_name=account_name, sas_token= sas_token)\n",
    "            from_blob = block_blob_service.get_blob_to_text(container_name = container_name, blob_name=blob_name)\n",
    "            return pd.read_csv(StringIO(from_blob.content))\n",
    "        input_df=get_data_from_blob(sas_token, account_name, container_name, blob_name)\n",
    "        #filtering out empty columns\n",
    "        input_df = input_df[input_df.x2.notnull()]\n",
    "        input_df = input_df[input_df.x23.notnull()]\n",
    "        input_df = input_df[input_df.x33.notnull()]\n",
    "        input_df = input_df[input_df.x54.notnull()]\n",
    "        \n",
    "        #print(input_df)\n",
    "        return(input_df)\n",
    "    except Exception as error_msg:\n",
    "        logger.info(f\"Exception occured while reading the dataset: {blob_name}\"\n",
    "                    f\"Error Info is  {error_msg}\")\n",
    "\n",
    "\n",
    "def get_productReturnQuantity():\n",
    "    logger.info(\"Calculating Quantity of product returned\")\n",
    "    dframe = read_dataset(\"ProductReturns.csv\") #*test#\n",
    "    productReturnQuantity = dframe['ReturnQuantity'].sum()\n",
    "    metric = {}\n",
    "    metric['title'] = \"Quantity of Products Returned\"\n",
    "    metric['value'] = str(int(productReturnQuantity)) + ' units'\n",
    "    logger.info(\"Successfully calculated Quantity of product returned\")\n",
    "    return metric\n",
    "\n",
    "\n",
    "kpi_json = get_productReturnQuantity()\n",
    "dynamic_outputs = json.dumps(kpi_json)\n",
    "\"\"\"\n",
    "#END CUSTOM CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BEGIN CUSTOM CODE BELOW...\n",
    "\n",
    "#put your output in this response param for connecting to downstream widgets\n",
    "#productReturn_UniqueProduct = \"\"\"\n",
    "# Below codestring is used to display the nuumber of uniue product types returned in a year.\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "def getLogger():\n",
    "    import logging\n",
    "    logging.basicConfig(filename=\"UIACLogger.log\",\n",
    "                        format='%(asctime)s %(message)s',\n",
    "                        filemode='a')\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    return logger\n",
    "\n",
    "\n",
    "logger = getLogger()\n",
    "\n",
    "\n",
    "def read_dataset(filename):\n",
    "    # Read dataset from the github.\n",
    "    logger.info(f\"Read dataset file: {filename}\")\n",
    "    try:\n",
    "        dataset_url = f\"https://raw.githubusercontent.com/saipraneeth4/codx_dataset/main/dataset/{filename}\"\n",
    "        dframe = pd.read_csv(dataset_url)\n",
    "        return dframe\n",
    "    except Exception as error_msg:\n",
    "        logger.info(f\"Exception occured while reading the dataset: {dataset_url}\"\n",
    "                    f\"Error Info is  {error_msg}\")\n",
    "\n",
    "\n",
    "def get_uniqueProductReturned():\n",
    "    logger.info(\"Calculating number of unique products returned\")\n",
    "    dframe = read_dataset(\"ProductReturns.csv\")\n",
    "    uniqueProductQuantity = len(dframe['ProductName'].unique())\n",
    "    metric = {}\n",
    "    metric['title'] = \"No of Unique Products Returned\"\n",
    "    metric['value'] = str(int(uniqueProductQuantity)) + ' unique Products'\n",
    "    logger.info(\"Successfully calculated number of unique products returned\")\n",
    "    return metric\n",
    "\n",
    "\n",
    "kpi_json = get_uniqueProductReturned()\n",
    "dynamic_outputs = json.dumps(kpi_json)\n",
    "\"\"\"\n",
    "#END CUSTOM CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BEGIN CUSTOM CODE BELOW...\n",
    "\n",
    "#put your output in this response param for connecting to downstream widgets\n",
    "# productReturns_costOfProductReturned = \"\"\"\n",
    "# Below codestring is used to display the total cost of product that was returned in a year.\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "def getLogger():\n",
    "    import logging\n",
    "    logging.basicConfig(filename=\"UIACLogger.log\",\n",
    "                        format='%(asctime)s %(message)s',\n",
    "                        filemode='a')\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    return logger\n",
    "\n",
    "\n",
    "logger = getLogger()\n",
    "\n",
    "\n",
    "def read_dataset(filename):\n",
    "    # Read dataset from the github.\n",
    "    logger.info(f\"Read dataset file: {filename}\")\n",
    "    try:\n",
    "        dataset_url = f\"https://raw.githubusercontent.com/saipraneeth4/codx_dataset/main/dataset/{filename}\"\n",
    "        dframe = pd.read_csv(dataset_url)\n",
    "        return dframe\n",
    "    except Exception as error_msg:\n",
    "        logger.info(f\"Exception occured while reading the dataset: {dataset_url}\"\n",
    "                    f\"Error Info is  {error_msg}\")\n",
    "\n",
    "\n",
    "def get_costOfProductReturned():\n",
    "    logger.info(\"Calculating Totalcost of products returned\")\n",
    "    dframe = read_dataset(\"ProductReturns.csv\")\n",
    "    costOfProdRetuned = dframe['ProductPrice'].sum()\n",
    "    metric = {}\n",
    "    metric['title'] = \"No of Unique Products Returned\"\n",
    "    metric['value'] = str(int(costOfProdRetuned)) + ' $'\n",
    "    logger.info(\"Successfully calculated Totalcost of products returned\")\n",
    "    return metric\n",
    "\n",
    "\n",
    "kpi_json = get_costOfProductReturned()\n",
    "dynamic_outputs = json.dumps(kpi_json)\n",
    "\"\"\"\n",
    "#END CUSTOM CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BEGIN CUSTOM CODE BELOW...\n",
    "\n",
    "#put your output in this response param for connecting to downstream widgets\n",
    "# orderDetailQuantity = \"\"\"\n",
    "# Below codestring is used to perform detailed analysis on quantity of sales done over time. \n",
    "\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import json\n",
    "import plotly.io as io\n",
    "\n",
    "\n",
    "def getLogger():\n",
    "    import logging\n",
    "    logging.basicConfig(filename=\"UIACLogger.log\",\n",
    "                        format='%(asctime)s %(message)s',\n",
    "                        filemode='a')\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    return logger\n",
    "\n",
    "\n",
    "logger = getLogger()\n",
    "\n",
    "\n",
    "def read_dataset(filename):\n",
    "    # Read dataset from the github.\n",
    "    logger.info(f\"Read dataset file: {filename}\")\n",
    "    try:\n",
    "        dataset_url = f\"https://raw.githubusercontent.com/saipraneeth4/codx_dataset/main/dataset/{filename}\"\n",
    "        dframe = pd.read_csv(dataset_url)\n",
    "        return dframe\n",
    "    except Exception as error_msg:\n",
    "        logger.info(f\"Exception occured while reading the dataset: {dataset_url}\"\n",
    "                    f\"Error Info is  {error_msg}\")\n",
    "\n",
    "\n",
    "def getGraph(dframe, filters):\n",
    "    logger.info(\n",
    "        \"Preparing bar graph json to understand products sales over time\")\n",
    "    for item in filters:\n",
    "        if 'All' in filters[item]:\n",
    "            continue\n",
    "        elif isinstance(filters[item], list):\n",
    "            dframe = dframe[dframe[item].isin(filters[item])]\n",
    "        else:\n",
    "            dframe = dframe[dframe[item] == filters[item]]\n",
    "    fig = px.bar(dframe, x='OrderDate', y='OrderQuantity', color='ProductName')\n",
    "    # fig.show()\n",
    "    logger.info(\n",
    "        \"Successfully prepared bar graph json to understand products sales over time\")\n",
    "    return io.to_json(fig)\n",
    "\n",
    "\n",
    "# selected_filters = {\"Region\": 'Australia'}\n",
    "dframe = read_dataset(\"ProductSales.csv\")\n",
    "dynamic_outputs = getGraph(dframe, selected_filters)\n",
    "\n",
    "\"\"\"\n",
    "#END CUSTOM CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BEGIN CUSTOM CODE BELOW...\n",
    "\n",
    "#put your output in this response param for connecting to downstream widgets\n",
    "# oderDetailProduct = \"\"\"\n",
    "# Below codestring is used to perform detailed analysis of Product stock available over time. \n",
    "\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import json\n",
    "import plotly.io as io\n",
    "\n",
    "\n",
    "def getLogger():\n",
    "    import logging\n",
    "    logging.basicConfig(filename=\"UIACLogger.log\",\n",
    "                        format='%(asctime)s %(message)s',\n",
    "                        filemode='a')\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    return logger\n",
    "\n",
    "\n",
    "logger = getLogger()\n",
    "\n",
    "\n",
    "def read_dataset(filename):\n",
    "    # Read dataset from the github.\n",
    "    logger.info(f\"Read dataset file: {filename}\")\n",
    "    try:\n",
    "        dataset_url = f\"https://raw.githubusercontent.com/saipraneeth4/codx_dataset/main/dataset/{filename}\"\n",
    "        dframe = pd.read_csv(dataset_url)\n",
    "        return dframe\n",
    "    except Exception as error_msg:\n",
    "        logger.info(f\"Exception occured while reading the dataset: {dataset_url}\"\n",
    "                    f\"Error Info is  {error_msg}\")\n",
    "\n",
    "\n",
    "def getGraph(dframe, filters):\n",
    "    logger.info(\n",
    "        \"Preparing bar graph json to understand products in stock available over time\")\n",
    "    for item in filters:\n",
    "        if 'All' in filters[item]:\n",
    "            continue\n",
    "        elif isinstance(filters[item], list):\n",
    "            dframe = dframe[dframe[item].isin(filters[item])]\n",
    "        else:\n",
    "            dframe = dframe[dframe[item] == filters[item]]\n",
    "    fig = px.bar(dframe, x='StockDate', y='OrderQuantity', color='ProductName')\n",
    "    # fig.show()\n",
    "    logger.info(\n",
    "        \"Successfully prepared bar graph json to understand products in stock available over time\")\n",
    "    return io.to_json(fig)\n",
    "\n",
    "\n",
    "selected_filters = {\"Region\": 'Australia'}\n",
    "dframe = read_dataset(\"ProductSales.csv\")\n",
    "dynamic_outputs = getGraph(dframe, selected_filters)\n",
    "\"\"\"\n",
    "#END CUSTOM CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BEGIN CUSTOM CODE BELOW...\n",
    "\n",
    "#put your output in this response param for connecting to downstream widgets\n",
    "#productReturns_gridTable = \"\"\"\n",
    "# Below codestring is used to view the product returns details.\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "def getLogger():\n",
    "    import logging\n",
    "    logging.basicConfig(filename=\"UIACLogger.log\",\n",
    "                        format='%(asctime)s %(message)s',\n",
    "                        filemode='a')\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    return logger\n",
    "\n",
    "\n",
    "logger = getLogger()\n",
    "\n",
    "\n",
    "def read_dataset(filename):\n",
    "    # Read dataset from the github.\n",
    "    logger.info(f\"Read dataset file: {filename}\")\n",
    "    try:\n",
    "        dataset_url = f\"https://raw.githubusercontent.com/saipraneeth4/codx_dataset/main/dataset/{filename}\"\n",
    "        dframe = pd.read_csv(dataset_url)\n",
    "        return dframe\n",
    "    except Exception as error_msg:\n",
    "        logger.info(f\"Exception occured while reading the dataset: {dataset_url}\"\n",
    "                    f\"Error Info is  {error_msg}\")\n",
    "\n",
    "\n",
    "def generate_dynamic_table(dframe, name='Sales', grid_options={\"tableSize\": \"small\", \"tableMaxHeight\": \"80vh\", \"quickSearch\":True}, group_headers=[], grid=\"auto\"):\n",
    "    logger.info(\"Generate dynamic Grid table json from dframe\")\n",
    "    table_dict = {}\n",
    "    table_props = {}\n",
    "    table_dict.update({\"grid\": grid, \"type\": \"tabularForm\",\n",
    "                      \"noGutterBottom\": True, 'name': name})\n",
    "    values_dict = dframe.dropna(axis=1).to_dict(\"records\")\n",
    "    table_dict.update({\"value\": values_dict})\n",
    "    col_def_list = []\n",
    "    for col in list(dframe.columns):\n",
    "        col_def_dict = {}\n",
    "        col_def_dict.update({\"headerName\": col, \"field\": col})\n",
    "        col_def_list.append(col_def_dict)\n",
    "    table_props[\"groupHeaders\"] = group_headers\n",
    "    table_props[\"coldef\"] = col_def_list\n",
    "    table_props[\"gridOptions\"] = grid_options\n",
    "    table_dict.update({\"tableprops\": table_props})\n",
    "    logger.info(\"Successfully generated dynamic Grid table json from dframe\")\n",
    "    return table_dict\n",
    "\n",
    "\n",
    "def build_grid_table_json():\n",
    "    logger.info(\"Preparing grid table json for Product Returns Screen\")\n",
    "    form_config = {}\n",
    "    dframe = read_dataset(\"ProductReturns.csv\")\n",
    "    form_config['fields'] = [generate_dynamic_table(dframe)]\n",
    "    grid_table_json = {}\n",
    "    grid_table_json['form_config'] = form_config\n",
    "    logger.info(\"Successfully prepared grid table json for Product Returns Screen\")\n",
    "    return grid_table_json\n",
    "\n",
    "\n",
    "grid_table_json = build_grid_table_json()\n",
    "dynamic_outputs = json.dumps(grid_table_json)\n",
    "\"\"\"\n",
    "#END CUSTOM CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BEGIN CUSTOM CODE BELOW...\n",
    "\n",
    "#put your output in this response param for connecting to downstream widgets\n",
    "#orderDetailFilter = \"\"\"\n",
    "## Below codestring is used to create filters to show different regions data in  Order Detail Analysis screen.\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "def getLogger():\n",
    "    import logging\n",
    "    logging.basicConfig(filename=\"UIACLogger.log\",\n",
    "                        format='%(asctime)s %(message)s',\n",
    "                        filemode='a')\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    return logger\n",
    "\n",
    "\n",
    "logger = getLogger()\n",
    "\n",
    "\n",
    "def read_dataset(filename):\n",
    "    # Read dataset from the github.\n",
    "    logger.info(f\"Read dataset file: {filename}\")\n",
    "    try:\n",
    "        dataset_url = f\"https://raw.githubusercontent.com/saipraneeth4/codx_dataset/main/dataset/{filename}\"\n",
    "        dframe = pd.read_csv(dataset_url)\n",
    "        return dframe\n",
    "    except Exception as error_msg:\n",
    "        logger.info(f\"Exception occured while reading the dataset: {dataset_url}\"\n",
    "                    f\"Error Info is  {error_msg}\")\n",
    "\n",
    "\n",
    "def get_response_filters(current_filter_params, df, default_values_selected, all_filters, multi_select_filters, extra_filters={}):\n",
    "    logger.info(\"Preparing filter dictionary\")\n",
    "    # Usage\n",
    "    # -----\n",
    "    # >>> filter_df = pd.DataFrame(columns=[....])    # Optional operation\n",
    "    # >>> filter_df = final_ADS.groupby(......)       # Optional operation\n",
    "    # >>> default_values_selected = {}    # The default value to be selected for a filter, provide filter_name, filter_values\n",
    "    # >>> all_option_filters = []         # Filters with an All option\n",
    "    # >>> multi_select_filters = []       # Filters with an multi_select option\n",
    "    # >>> more_filters = {}               # Extra filters, provide filter_names, filter_options\n",
    "    # >>> final_dict_out = get_response_filters(current_filter_params, filter_df, default_values_selected, all_option_filters, multi_select_filters, more_filters)\n",
    "    # >>> dynamic_outputs = json.dumps(final_dict_out)\n",
    "    # Returns\n",
    "    # -------\n",
    "    # A dict object containing the filters JSON structure\n",
    "\n",
    "    filters = list(df.columns)\n",
    "    default_values_possible = {}\n",
    "    for item in filters:\n",
    "        default_possible = list(df[item].unique())\n",
    "        if item in all_filters:\n",
    "            default_possible = list(chain(['All'], default_possible))\n",
    "        default_values_possible[item] = default_possible\n",
    "    if extra_filters:\n",
    "        filters.extend(list(extra_filters.keys()))\n",
    "        default_values_possible.update(extra_filters)\n",
    "    if current_filter_params:\n",
    "        selected_filters = current_filter_params[\"selected\"]\n",
    "        # current_filter = current_filter_params[\"current_filter\"]\n",
    "        # current_index = filters.index(current_filter)\n",
    "        select_df = df.copy()\n",
    "    final_dict = {}\n",
    "    iter_value = 0\n",
    "    data_values = []\n",
    "    default_values = {}\n",
    "    for item in filters:\n",
    "        filter_dict = {}\n",
    "        filter_dict[\"widget_filter_index\"] = int(iter_value)\n",
    "        filter_dict[\"widget_filter_function\"] = False\n",
    "        filter_dict[\"widget_filter_function_parameter\"] = False\n",
    "        filter_dict[\"widget_filter_hierarchy_key\"] = False\n",
    "        filter_dict[\"widget_filter_isall\"] = True if item in all_filters else False\n",
    "        filter_dict[\"widget_filter_multiselect\"] = True if item in multi_select_filters else False\n",
    "        filter_dict[\"widget_tag_key\"] = str(item)\n",
    "        filter_dict[\"widget_tag_label\"] = str(item)\n",
    "        filter_dict[\"widget_tag_input_type\"] = \"select\",\n",
    "        filter_dict[\"widget_filter_dynamic\"] = True\n",
    "        if current_filter_params:\n",
    "            if item in df.columns:\n",
    "                possible_values = list(select_df[item].unique())\n",
    "                item_default_value = selected_filters[item]\n",
    "                if item in all_filters:\n",
    "                    possible_values = list(chain(['All'], possible_values))\n",
    "                if item in multi_select_filters:\n",
    "                    for value in selected_filters[item]:\n",
    "                        if value not in possible_values:\n",
    "                            if possible_values[0] == \"All\":\n",
    "                                item_default_value = possible_values\n",
    "                            else:\n",
    "                                item_default_value = [possible_values[0]]\n",
    "                else:\n",
    "                    if selected_filters[item] not in possible_values:\n",
    "                        item_default_value = possible_values[0]\n",
    "                filter_dict[\"widget_tag_value\"] = possible_values\n",
    "                if item in multi_select_filters:\n",
    "                    if 'All' not in item_default_value and selected_filters[item]:\n",
    "                        select_df = select_df[select_df[item].isin(\n",
    "                            item_default_value)]\n",
    "                else:\n",
    "                    if selected_filters[item] != 'All':\n",
    "                        select_df = select_df[select_df[item]\n",
    "                                              == item_default_value]\n",
    "            else:\n",
    "                filter_dict[\"widget_tag_value\"] = extra_filters[item]\n",
    "        else:\n",
    "            filter_dict[\"widget_tag_value\"] = default_values_possible[item]\n",
    "            item_default_value = default_values_selected[item]\n",
    "        data_values.append(filter_dict)\n",
    "        default_values[item] = item_default_value\n",
    "        iter_value = iter_value + 1\n",
    "    final_dict[\"dataValues\"] = data_values\n",
    "    final_dict[\"defaultValues\"] = default_values\n",
    "    logger.info(\"Successfully prepared filter dictionary\")\n",
    "    return final_dict\n",
    "\n",
    "\n",
    "def prepare_filter_json():\n",
    "    logger.info(f\"Preparing json for Filters in Order Detail Screen\")\n",
    "    # Preapre Filter json for Region in the Order Detail View Screen.\n",
    "    dframe = read_dataset(\"ProductSales.csv\")\n",
    "    dframe = dframe.groupby(['Region']).sum().reset_index()\n",
    "    filter_dframe = dframe[['Region']]\n",
    "    default_values_selected = {\"Region\": 'Australia'}\n",
    "    all_filters = []\n",
    "    multi_select_filters = []\n",
    "    # current_filter_params = {\"selected\": default_values_selected}\n",
    "    final_dict_out = get_response_filters(\n",
    "        current_filter_params, filter_dframe, default_values_selected, all_filters, multi_select_filters)\n",
    "    logger.info(f\"Successful prepared json for Filters in Order Detail Screen\")\n",
    "    return json.dumps(final_dict_out)\n",
    "\n",
    "\n",
    "dynamic_outputs = prepare_filter_json()\n",
    "\n",
    "\"\"\"\n",
    "#END CUSTOM CODE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.5 (tags/v3.7.5:5c02a39a0b, Oct 15 2019, 00:11:34) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a96afb4c5e530be632fe4ae220902658aaf598ffb07f39eceab225b5ab973af6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
